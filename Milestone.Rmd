---
title: "Text Prediction [PART 2] - EDA "
author: "Shounak Shastri"
date: "19/08/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
suppressPackageStartupMessages({
    #Ì¥library(ngram)
    library(stringr)
    library(tidytext)
    library(textclean)
    library(tibble)
    library(ggplot2)
    library(ggpubr)
    library(dplyr)})
```


## Introduction

This report summarizes the Exploratory Data Analysis performed on a text dataset. The dataset is included in the github repo and is downloaded from the swiftkey This analysis will help us plan the development of an app which would predict the next word in a sentence like the autocomplete feature in most mobile phones.

Initial set up
---

I have already downloaded the file through the link provided. So, here we will set the working directory and import the data.
First we read the data into separate variables and close the connections to save memory.

```{r, echo = TRUE}
setwd("C:\\Users\\Intel\\Documents\\Coursera DS Capstone Project")

# Import all the files and open the connection
fileBlogs <- file(".\\final\\en_US\\en_US.blogs.txt", "rb")
fileNews <- file(".\\final\\en_US\\en_US.news.txt", "rb")
fileTweets <- file(".\\final\\en_US\\en_US.twitter.txt", "rb")

# Read the lines and close the connections
blogs <- readLines(fileBlogs, encoding = "UTF-8", skipNul = TRUE)
close(fileBlogs)

news <- readLines(fileNews, encoding = "UTF-8", skipNul = TRUE)
close(fileNews)

tweets <- readLines(fileTweets, encoding = "UTF-8", skipNul = TRUE)
close(fileTweets)

# Remove the variables from the workspace
rm(fileBlogs, fileNews, fileTweets)
```

Basic summary of the data
---

Let's check the memory utilization by the individual files in order to get some perspective about the space requirements. We will also use run the garbage collector to free up some space for R.  We will do `gc()` everytime we remove some big variables.

```{r, echo = TRUE}
blogsMem <- object.size(blogs)
format(blogsMem, units = "MB", standard = "legacy")

newsMem <- object.size(news)
format(newsMem, units = "MB", standard = "legacy")

tweetsMem <- object.size(tweets)
format(tweetsMem, units = "MB", standard = "legacy")

totalMem <- blogsMem + newsMem + tweetsMem
format(totalMem, units = "MB", standard = "legacy")

rm(blogsMem, newsMem, tweetsMem, totalMem)
gc() # Garbage Collector
```

So, the dataset needs about 832MB of RAM. The `gc()` output also indicates that the max memory used is around 1.9Gb. This might create some problems while creating the application as the shiny website only provides 1Gb of space for the app data. Anything above that would require a purchase of premium plans. So it would be better if we design the app to the take data in chunks instead of the whole file.


```{r, echo = TRUE}
basicSummary <- data.frame(fileType = c("blogs", "news", "twitter"), 
                           nlines = c(length(blogs), length(news), length(tweets)),
                           nwords = c(wordcount(blogs,sep = " "), 
                                      wordcount(news, sep = " "), 
                                      wordcount(tweets, sep = " ")),
                           longestLine = c(max(nchar(blogs)), max(nchar(news)), max(nchar(tweets))))
basicSummary
```

It should be noted here that the longest lines in the twitter files are 140 characters in length. This is accurate as tweets are limited to 140 characters. It can also be observed here that, even though the twitter dataset is largest in terms of number of lines and memory required, the number of words is much less than that in blogs. This indicates a usage of shorter senteneces or, more probably, special characters like emojis. 

Cleaning and Sampling the data 
---

In this section, we will clean the data and create smaller samples so that it is easier to work on. The cleaning process will involve the following steps:
 - Expanding contractions
 - converting all characters to lowercase
 - removing digits and words containing digits
 - removing punctuations

### Converting the files to tibbles (basic data frames)

```{r, echo=TRUE}
blogs <- tibble(text = blogs)
news <- tibble(text = news)
tweets <- tibble(text = tweets)
```

## Combining the data and creating samples

Here, we first combine all the three dataframes into one and then sample from that text. This combination is done by using the `bind_rows()` function. We add an additional column to the new dataframe which tells us the source of the row. This is done using the `mutate()` function.

```{r, echo=TRUE}
corpus <- bind_rows(blogs, news, tweets)
rm(blogs, news, tweets)
gc()
```

### Creating a sample

Now we will create a sample from the combined data and operate on that. This will make our operations run faster as we don't have to use the complete dataset. The `set.seed()` line should be uncommented for reproducability. Also, from this point on, we will be operating on the sample, so we will remove the original dataset to free up some memory.

```{r, echo = TRUE}
set.seed(5)
corpusSample <- corpus[sample(nrow(corpus), 10000), ]
rm(corpus)
```

### Tidying the sample dataset

Here we will remove all the contractions, numbers, punctuation, special characters and emoticons from the sample.

```{r, echo=TRUE}
corpusSample$text <- replace_contraction(corpusSample$text)
corpusSample$text <- gsub("\\d", "", corpusSample$text) # Remove Numbers
corpusSample$text <- gsub("[^\x01-\x7F]", "", corpusSample$text) # Remove emoticons
corpusSample$text <- gsub("[^[:alnum:]]", " ", corpusSample$text) # Remove special characters. Adds extra spaces
corpusSample$text <- gsub("\\s+", " ", corpusSample$text) # Remove the extra spaces
```

## Tokenizing the sample

TOkenizing means to separate the words from the string. Here, we will create two sets of tidy data, one which includes all the stopwords and one with the stopwords removed. Stop words is a list of words commonly used in the English language which are usually removed from the text while implementing text mining applications. The `tidytext` package comes with a built-in list `stop_words` which can be seen below.

```{r, echo = FALSE}
head(stop_words)
```

Although removing the stopwords would reduce the load on our system significantly, it is not advisable to do so in the case of our target application as discussed in Part 1. That is because the text predictor should be able to predict words included in the list.

```{r, echo = TRUE}
tidyset_withStopWords <- corpusSample %>%
    unnest_tokens(word, text)

data("stop_words")
tidyset_withoutStopWords <- corpusSample %>%
    unnest_tokens(word, text) %>%
    anti_join(stop_words)
```

Lets see how many unique words are there in both sets

```{r,echo= TRUE}
keyWithStopwords <- unique(tidyset_withStopWords)
keyWithoutStopwords <- unique(tidyset_withoutStopWords)
dim(keyWithStopwords)
dim(keyWithoutStopwords)
```


Answering the questions
---

- How many words are required to attain 50% coverage of all the words in the sample?

```{r, echo=TRUE}
coverage50pctWithStopwords <- tidyset_withStopWords %>%
    count(word) %>%  
    mutate(proportion = n / sum(n)) %>%
    arrange(desc(proportion)) %>%  
    mutate(coverage = cumsum(proportion)) %>%
    filter(coverage <= 0.5)
nrow(coverage50pctWithStopwords)
```


- How many words are required to attain 90% coverage of all the words in the sample?

```{r, echo=TRUE}
coverage90pctWithStopwords <- tidyset_withStopWords %>%
    count(word) %>%  
    mutate(proportion = n / sum(n)) %>%
    arrange(desc(proportion)) %>%  
    mutate(coverage = cumsum(proportion)) %>%
    filter(coverage <= 0.9)
nrow(coverage90pctWithStopwords)
```

## Plotting the distributions

Here we display the plots for uni, bi and trigrams for both with and without stopwords.

### Unigrams

```{r, echo=TRUE}
coverage90pctWithStopwords %>%
    top_n(20, proportion) %>%
    mutate(word = reorder(word, proportion)) %>%
    ggplot(aes(word, proportion)) +
    geom_col() +
    xlab("Words") +
    ggtitle("Unigram Distribution for 90% coverage with Stopwords")+
    theme(plot.title = element_text(hjust = 0.5))+
    coord_flip()

rm(tidyset_withStopWords,
   keyWithStopwords, 
   coverage50pctWithStopwords,
   coverage90pctWithStopwords)
```

### Bigrams

```{r, echo = TRUE}

tidyset_withStopWords <- corpusSample %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2)


coverage90pctWithStopwords <- tidyset_withStopWords %>%
    count(bigram) %>%  
    mutate(proportion = n / sum(n)) %>%
    arrange(desc(proportion)) %>%  
    mutate(coverage = cumsum(proportion)) %>%
    filter(coverage <= 0.9)

coverage90pctWithStopwords %>%
    top_n(20, proportion) %>%
    mutate(bigram = reorder(bigram, proportion)) %>%
    ggplot(aes(bigram, proportion)) +
    geom_col() +
    xlab("Bigrams") +
    ggtitle("Bigram Distribution for 90% coverage")+
    theme(plot.title = element_text(hjust = 0.5))+
    coord_flip()

rm(tidyset_withStopWords, 
   coverage90pctWithStopwords)

```


### Trigrams

```{r, echo = TRUE}

tidyset_withStopWords <- corpusSample %>%
    unnest_tokens(trigram, text, token = "ngrams", n = 3)


coverage90pctWithStopwords <- tidyset_withStopWords %>%
    count(trigram) %>%  
    mutate(proportion = n / sum(n)) %>%
    arrange(desc(proportion)) %>%  
    mutate(coverage = cumsum(proportion)) %>%
    filter(coverage <= 0.9)

coverage90pctWithStopwords %>%
    top_n(20, proportion) %>%
    mutate(trigram = reorder(trigram, proportion)) %>%
    ggplot(aes(trigram, proportion)) +
    geom_col() +
    xlab("Trigrams") +
    ggtitle("Trigram Distribution for 90% coverage")+
    theme(plot.title = element_text(hjust = 0.5))+
    coord_flip()

rm(tidyset_withStopWords, 
   coverage90pctWithStopwords)
```