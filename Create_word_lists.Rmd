---
title: "Word Prediction Model - PART 3 - Model Building"
collection: NLP
type: "Word Prediction Model"
permalink: /NLP/Word Prediction Model - PART 2 - Model Building
tags: 
    - Machine Learning
    - NLP
    - R
    - Predictive Text
    - n-grams
date: 2020-10-20
output: 
    html_document: 
      keep_md: TRUE
---

## Introduction

This is PART 3 of our project in which we try to make an application which predicts text. In PART 1, we got our text data and did a basic exploration. We found that the amount of data is quite large and the local machine is not really sufficient in terms of the available RAM. The available RAM runs at 90-95% capacity during the tokenization process. 


Import all the necessary libraries

```{r setup, include=FALSE}
rm(list = ls())
suppressPackageStartupMessages({
    library(tidytext)
    library(textclean)
    library(tibble)
    library(dplyr)
    library(tidyr)})
```

We will use the same steps as in Part 2 to import the files.

```{r}
setwd("C:\\Users\\shoun\\Documents\\Coursera DS Capstone Project")

# Import all the files and open the connection
fileBlogs <- file(".\\final\\en_US\\en_US.blogs.txt", "rb")
fileNews <- file(".\\final\\en_US\\en_US.news.txt", "rb")
fileTweets <- file(".\\final\\en_US\\en_US.twitter.txt", "rb")

# Read the lines and close the connections
blogs <- readLines(fileBlogs, encoding = "UTF-8", skipNul = TRUE)
close(fileBlogs)

news <- readLines(fileNews, encoding = "UTF-8", skipNul = TRUE)
close(fileNews)

tweets <- readLines(fileTweets, encoding = "UTF-8", skipNul = TRUE)
close(fileTweets)

# Remove the variables from the workspace
rm(fileBlogs, fileNews, fileTweets)
```

As seen from the EDA in PART 2, my computer doesn't have enough RAM to handle the data at once. So, we will break the corpus into samples of a reasonable size and work on that sample, for now. 

```{r,echo = TRUE}
blogs <- tibble(text = blogs[1:120000])
news <- tibble(text = news[1:120000])
tweets <- tibble(text = tweets[1:120000])
```

Here, I have taken the first 80k entries from the blogs, news and tweets files. I can take the rest after I process these. I would have to run this program multiple times to generate a model that has been trained on the complete dataset. 


Now, we  do some preprocessing on our sample data. The basic steps include replacing contractions, converting all the letters to lowercase, removing numbers, emoticons, special characters like punctuations, etc and removing extra spaces. We use `regex` for this. 

```{r}
blogs$text <- replace_contraction(blogs$text)
blogs$text <- tolower(blogs$text)
blogs$text <- gsub("\\d", "", blogs$text) # Remove Numbers
blogs$text <- gsub("[^\x01-\x7F]", "", blogs$text) # Remove emoticons
blogs$text <- gsub("[^[:alnum:]]", " ", blogs$text) # Remove special characters. Adds extra spaces
blogs$text <- gsub("\\s+", " ", blogs$text) # Remove the extra spaces

news$text <- replace_contraction(news$text)
news$text <- tolower(news$text)
news$text <- gsub("\\d", "", news$text) # Remove Numbers
news$text <- gsub("[^\x01-\x7F]", "", news$text) # Remove emoticons
news$text <- gsub("[^[:alnum:]]", " ", news$text) # Remove special characters. Adds extra spaces
news$text <- gsub("\\s+", " ", news$text) # Remove the extra spaces

tweets$text <- replace_contraction(tweets$text)
tweets$text <- tolower(tweets$text)
tweets$text <- gsub("\\d", "", tweets$text) # Remove Numbers
tweets$text <- gsub("[^\x01-\x7F]", "", tweets$text) # Remove emoticons
tweets$text <- gsub("[^[:alnum:]]", " ", tweets$text) # Remove special characters. Adds extra spaces
tweets$text <- gsub("\\s+", " ", tweets$text) # Remove the extra spaces
```

After this step, we simply combine all the three types of data into one table. This would give us a good variety in the type of sentences we are working with.

```{r}
corpus <- rbind(blogs, news, tweets)
rm(blogs, news, tweets)
gc()
```

Now, here, we create the ngrams from our data and store them into easily accessible files. The main reason for doing this is to reduce the load on our hardware during deployment.

```{r}
# Creating a list of bigrams

bigrams<- corpus %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2)

bigramSet <- bigrams %>%
    count(bigram) %>%
    filter(n > 6)
rm(list = c("bigrams"))

biwords <- bigramSet %>%
    separate(bigram, c("word1", "word2"), sep = " ")
rm(bigramSet)

saveRDS(biwords, "./biwords.rds")
rm(biwords)
```


```{r}
# Creating a list of trigrams

trigrams<- corpus %>%
    unnest_tokens(trigram, text, token = "ngrams", n = 3)

trigramSet <- trigrams %>%
    count(trigram) %>%
    filter(n > 6)
rm(list = c("trigrams"))

triwords <- trigramSet %>%
    separate(trigram, c("word1", "word2", "word3"), sep = " ")
rm(trigramSet)
saveRDS(triwords, "./triwords.rds")
rm(triwords)
```


```{r}
# Creaating a list of quadgrams

quadgrams<- corpus %>%
    unnest_tokens(quadgram, text, token = "ngrams", n = 4)

quadgramSet <- quadgrams %>%
    count(quadgram) %>%
    filter(n > 6)
rm(list = c("quadgrams"))

quadwords <- quadgramSet %>%
    separate(quadgram, c("word1", "word2", "word3", "word4"), sep = " ")
rm(quadgramSet)
saveRDS(quadwords, "./quadwords.rds")
rm(quadwords)
```

```{r}
# Creating a list of pentagrams

pentagrams<- corpus %>%
    unnest_tokens(pentagram, text, token = "ngrams", n = 5)

pentagramSet <- pentagrams %>%
    count(pentagram) %>%
    filter(n > 6)
rm(list = c("pentagrams"))
pentawords <- pentagramSet %>%
    separate(pentagram, c("word1", "word2", "word3", "word4", "word5"), sep = " ")
rm(pentagramSet)
saveRDS(pentawords, "./pentawords.rds")
rm(pentawords)

rm(corpus)
gc()
```


