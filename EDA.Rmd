---
title: "Word Prediction Model - PART 2 - Exploratory Data Analysis "
collection: NLP
type: "Word Prediction Model"
permalink: /NLP/Word Prediction Model - PART 2 - Exploratory Data Analysis
tags: 
    - Machine Learning
    - NLP
    - R
    - Exploratory Data Analysis
    - n-grams
date: 2020-10-15
output: 
    html_document: 
      keep_md: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
suppressPackageStartupMessages({
    library(ngram)
    library(stringr)
    library(tidytext)
    library(textclean)
    library(tibble)
    library(ggplot2)
    library(ggpubr)
    library(dplyr)})
setwd("C:\\Users\\Intel\\Documents\\Coursera DS Capstone Project")
```


## Introduction

This report summarizes the Exploratory Data Analysis performed on a text dataset. The dataset is included in the github repo and is downloaded from the swiftkey This analysis will help us plan the development of an app which would predict the next word in a sentence like the autocomplete feature in most mobile phones.

I have already downloaded the corpus while testing. So, here we will directly import the data and start doing some analysis.

## Initial set up
---

First we read the data into separate variables and close the connections to save memory.

```{r, echo = TRUE}

# Import all the files and open the connection
fileBlogs <- file(".\\final\\en_US\\en_US.blogs.txt", "rb")
fileNews <- file(".\\final\\en_US\\en_US.news.txt", "rb")
fileTweets <- file(".\\final\\en_US\\en_US.twitter.txt", "rb")

# Read the lines and close the connections
blogs <- readLines(fileBlogs, encoding = "UTF-8", skipNul = TRUE)
close(fileBlogs)
rm(fileBlogs)

news <- readLines(fileNews, encoding = "UTF-8", skipNul = TRUE)
close(fileNews)
rm( fileNews)

tweets <- readLines(fileTweets, encoding = "UTF-8", skipNul = TRUE)
close(fileTweets)
rm(fileTweets)
```


Let's see what the data looks like. We will print the first 3 rows from all three files. I ran this code in RStudio, so I can see the size of the variables directly in the environment tab. But, in case you do not have access to the environment tab, we can use 

### Blogs
```{r, echo = FALSE}
head(blogs[1:3])
```

### News
```{r, echo = FALSE}
head(news[1:3])
```

### Tweets
```{r, echo = FALSE}
head(tweets[1:3])
```

Now by looking at the blogs, news and tweets, we can say that out of the three files, `news` is most likely to have more formal language and `tweets` is most likely to have an informal language. It probably won't affect the final application because we will be cleaning the data before training, but it's a good observation to make to distinguish between the three files.

## Basic summary of the data

First, let's check the memory utilization by the individual files in order to get some perspective about the space requirements. We will also use run the garbage collector to free up some space for R.  We will do `gc()` everytime we remove some big variables.

```{r, echo = FALSE}
blogsMem <- object.size(blogs)
format(blogsMem, units = "MB", standard = "legacy")

newsMem <- object.size(news)
format(newsMem, units = "MB", standard = "legacy")

tweetsMem <- object.size(tweets)
format(tweetsMem, units = "MB", standard = "legacy")

totalMem <- blogsMem + newsMem + tweetsMem
format(totalMem, units = "MB", standard = "legacy")

rm(blogsMem, newsMem, tweetsMem, totalMem)
gc() # Garbage Collector
```



```{r, echo = FALSE}
basicSummary <- data.frame(fileType = c("blogs", "news", "twitter"), 
                           nlines = c(length(blogs), length(news), length(tweets)),
                           nwords = c(wordcount(blogs,sep = " "), 
                                      wordcount(news, sep = " "), 
                                      wordcount(tweets, sep = " ")),
                           longestLine = c(max(nchar(blogs)), max(nchar(news)), max(nchar(tweets))))
basicSummary
```

So, the dataset needs about 832MB of memory to store in the current state. This might create some problems while creating the application as the shiny website only provides 1Gb of space for the app data. Anything above that would require a purchase of premium plans.  We did hint this in PART 1 of this series but, looking at this analysis, it is 
clear that it would be better if we design the app to use lesser space.

It should be noted here that the longest lines in the twitter files are 140 characters in length. This is accurate twiter limits all the tweets to 140 characters. Atleast this was the case a few years back. Now  It can also be observed here that, even though the twitter dataset is largest in terms of number of lines and memory required, the number of words is much less than that in blogs. This indicates a usage of shorter senteneces or, more probably, special characters like emojis. 

## Cleaning and Sampling the data 

In this section, we will clean the data and create smaller samples so that it is easier to work on. The cleaning process will involve the following steps:
 - Expanding contractions
 - converting all characters to lowercase
 - removing digits and words containing digits
 - removing punctuations

### Converting the files to tibbles (basic data frames)

```{r, echo=TRUE}
blogs <- tibble(text = blogs)
news <- tibble(text = news)
tweets <- tibble(text = tweets)
```

### Combining the data and creating samples

Here, we first combine all the three dataframes into one and then sample from that text. This combination is done by using the `bind_rows()` function. We add an additional column to the new dataframe which tells us the source of the row. This is done using the `mutate()` function. We combine all the three data frames so that when we create a sample, it would randomly pick rows from all three data frames combined. This would be great to analyse the n-gram distributions.

```{r, echo=TRUE, }
corpus <- bind_rows(blogs, news, tweets)
rm(blogs, news, tweets)
gc()
```

### Creating a sample

Now we will create a sample from the combined data and operate on that. This would give us a good approximation while making the code faster as we don't have to use the complete dataset. The `set.seed()` line should be uncommented for reproducability. Also, from this point on, we will be operating on the sample, so we will remove the original dataset to free up some memory.

```{r, echo = TRUE}
set.seed(5)
corpusSample <- corpus[sample(nrow(corpus), 50000), ]
rm(corpus)
```

## Tidying the sample dataset

Here we will remove all the contractions, numbers, punctuation, special characters and emoticons from the sample. We use regex to detect the patterns.

```{r, echo=TRUE}
corpusSample$text <- tolower(corpusSample$text) # Convert everything to lowercase
corpusSample$text <- replace_contraction(corpusSample$text)
corpusSample$text <- gsub("\\d", "", corpusSample$text) # Remove Numbers
corpusSample$text <- gsub("[^\x01-\x7F]", "", corpusSample$text) # Remove emoticons
corpusSample$text <- gsub("[^[:alnum:]]", " ", corpusSample$text) # Remove special characters. Adds extra spaces
corpusSample$text <- gsub("\\s+", " ", corpusSample$text) # Remove the extra spaces
```

## Tokenizing the sample

TOkenizing means to separate the words from the string. Here, we will create two sets of tidy data, one which includes all the stopwords and one with the stopwords removed. Stop words is a list of words commonly used in the English language which are usually removed from the text while implementing text mining applications. Although removing the stopwords would reduce the load on our system significantly, it is not advisable to do so in the case of our target application as discussed in Part 1. That is because the text predictor should be able to predict words included in the list. But, we will check it just to feed my personal curiosity.

```{r, echo = TRUE}
tidyset_withStopWords <- corpusSample %>%
    unnest_tokens(word, text)

data("stop_words")
tidyset_withoutStopWords <- corpusSample %>%
    unnest_tokens(word, text) %>%
    anti_join(stop_words)
```

Lets see how many unique words are there in both sets

```{r,echo= TRUE}
keyWithStopwords <- unique(tidyset_withStopWords)
keyWithoutStopwords <- unique(tidyset_withoutStopWords)
dim(keyWithStopwords)
dim(keyWithoutStopwords)
```

## Observations from the EDA

How many tokens can be used to cover half of all the words in the sample?

```{r, echo=TRUE}
coverage50pctWithStopwords <- tidyset_withStopWords %>%
    count(word) %>%  
    mutate(proportion = n / sum(n)) %>%
    arrange(desc(proportion)) %>%  
    mutate(coverage = cumsum(proportion)) %>%
    filter(coverage <= 0.5)
nrow(coverage50pctWithStopwords)
```


How many tokens can be used to cover 90% of all the words in the sample?

```{r, echo=TRUE}
coverage90pctWithStopwords <- tidyset_withStopWords %>%
    count(word) %>%  
    mutate(proportion = n / sum(n)) %>%
    arrange(desc(proportion)) %>%  
    mutate(coverage = cumsum(proportion)) %>%
    filter(coverage <= 0.9)
nrow(coverage90pctWithStopwords)
```

### Plotting the distributions

Here we display the plots for uni, bi and trigrams for the sample.

### Unigrams

```{r, echo=TRUE}
coverage90pctWithStopwords %>%
    top_n(20, proportion) %>%
    mutate(word = reorder(word, proportion)) %>%
    ggplot(aes(word, proportion)) +
    geom_col() +
    xlab("Words") +
    ggtitle("Unigram Distribution for 90% coverage with Stopwords")+
    theme(plot.title = element_text(hjust = 0.5))+
    coord_flip()

rm(tidyset_withStopWords,
   keyWithStopwords, 
   coverage50pctWithStopwords,
   coverage90pctWithStopwords)
```

### Bigrams

```{r, echo = TRUE}

tidyset_withStopWords <- corpusSample %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2)


coverage90pctWithStopwords <- tidyset_withStopWords %>%
    count(bigram) %>%  
    mutate(proportion = n / sum(n)) %>%
    arrange(desc(proportion)) %>%  
    mutate(coverage = cumsum(proportion)) %>%
    filter(coverage <= 0.9)

coverage90pctWithStopwords %>%
    top_n(20, proportion) %>%
    mutate(bigram = reorder(bigram, proportion)) %>%
    ggplot(aes(bigram, proportion)) +
    geom_col() +
    xlab("Bigrams") +
    ggtitle("Bigram Distribution for 90% coverage")+
    theme(plot.title = element_text(hjust = 0.5))+
    coord_flip()

rm(tidyset_withStopWords, 
   coverage90pctWithStopwords)

```


### Trigrams

```{r, echo = TRUE}

tidyset_withStopWords <- corpusSample %>%
    unnest_tokens(trigram, text, token = "ngrams", n = 3)


coverage90pctWithStopwords <- tidyset_withStopWords %>%
    count(trigram) %>%  
    mutate(proportion = n / sum(n)) %>%
    arrange(desc(proportion)) %>%  
    mutate(coverage = cumsum(proportion)) %>%
    filter(coverage <= 0.9)

coverage90pctWithStopwords %>%
    top_n(20, proportion) %>%
    mutate(trigram = reorder(trigram, proportion)) %>%
    ggplot(aes(trigram, proportion)) +
    geom_col() +
    xlab("Trigrams") +
    ggtitle("Trigram Distribution for 90% coverage")+
    theme(plot.title = element_text(hjust = 0.5))+
    coord_flip()

rm(tidyset_withStopWords, 
   coverage90pctWithStopwords)
```

Now that we know what we are dealing with, we can proceed to make to model